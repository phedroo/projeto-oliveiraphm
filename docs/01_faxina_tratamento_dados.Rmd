---
title: "Faxina e tratamento inicial"
author: "Oliveria, PHM; Panosso, AR"
date: "2025-05-07"
output: html_document
---

```{r setup, include=FALSE, eval = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      collapse = TRUE,
                      comment = "#>")
source("../R/my-function.R")
```

### Faxina e Processamento de Dados de CO₂ Atmosférico (XCO₂) do OCO-2/NASA

Objetivo

Este script realiza a leitura e pré-processamento de arquivos no formato `.nc4` (NetCDF) contendo dados de concentração de dióxido de carbono atmosférico (XCO₂) medidos pelo satélite OCO-2 da NASA. Os dados são filtrados para a região do Brasil e estruturados em um formato tabular para análises posteriores.

Identifica todos os arquivos com extensão .nc4 no diretório data-raw/nc4/ e retorna um vetor com os caminhos completos dos arquivos para leitura posterior.

```{r, eval = FALSE}
# Lista com caminho e nomes dos arquivo a serem lidos - xco2
list_of_files <- list.files("data-raw/nc4/",
                            pattern = ".nc4",
                            full.names = TRUE)
```

Utilizamos o pacote `{geobr}` para carregar o polígono do território brasileiro e extrai as coordenadas do polígono e converte para uma matriz (formato necessário para a função de filtro espacial). Após isso definimos uma função que verifica se um ponto (longitude, latitude) está dentro do polígono do Brasil por meio da utilização da função `point.in.polygon()` do pacote `{sp}` para a operação espacial. A função retorna retorna `TRUE/FALSE` para cada coordenada.

```{r, eval = FALSE}
## buscando o contorno do Brasil
br <- geobr::read_country(showProgress = FALSE)
pol_br <- br$geom |> purrr::pluck(1) |> as.matrix()
def_pol <- function(x, y, pol){
  as.logical(sp::point.in.polygon(point.x = x,
                                  point.y = y,
                                  pol.x = pol[,1],
                                  pol.y = pol[,2]))
}
```

Criação da **Função principal** para ler cada arquivo `.nc4` e extrair as variáveis essenciais:

+longitude, latitude: Coordenadas geográficas.

+time: Carimbo de tempo da medição.

+xco2: Concentração de CO₂ (partes por milhão - ppm).

+xco2_quality_flag e xco2_incerteza: Indicadores de qualidade e incerteza das medições.

+Fechar a conexão com o arquivo após a leitura (`nc_close()`).

```{r, eval = FALSE}
## função para ler arquivos NCD4
ncdf_reader <- function(file_path){
  nc_file <- ncdf4::nc_open(file_path)
  df <- data.frame(
    "longitude"=ncdf4::ncvar_get(nc_file,varid="longitude"),
    "latitude"=ncdf4::ncvar_get(nc_file,varid="latitude"),
    "time"=ncdf4::ncvar_get(nc_file,varid="time"),
    "xco2"=ncdf4::ncvar_get(nc_file,varid="xco2"),
    "xco2_quality_flag"=ncdf4::ncvar_get(nc_file,varid="xco2_quality_flag"),
    "xco2_incerteza"=ncdf4::ncvar_get(nc_file,varid="xco2_uncertainty"),
    "path" = file_path
  )
  ncdf4::nc_close(nc_file)
  return(df)
}
```

O código está processando um grande volume de arquivos NetCDF `(.nc4)` contendo dados de XCO2 e filtrando apenas as medições dentro da área geográfica de interesse do Brasil.

Como os arquivos podem ser muitos (e grandes!), o processo deve ser dividido em etapas para:

1-Evitar sobrecarregar a memória do R (lendo e processando em lotes).

2-Facilitar a depuração (se algo falhar, não se perde todo o progresso).

3-Permitir paralelização futura (cada lote poderia ser processado em núcleos diferentes).

`purrr::map_df`: Aplica a função `ncdf_reader` (que definimos antes) em $1000$ arquivos por vez e combina os resultados em um único `data.frame` e `write_rds` salva o resultado em disco no formato `.rds` (eficiente para objetos do R). Observer que se houver 10.000 arquivos, tentar ler todos de uma vez pode travar o R por falta de memória além de que, se ocorrer um erro, não perderemos o processamento já feito.

```{r, eval = FALSE}
dff_1 <- purrr::map_df(list_of_files[1:1000],ncdf_reader)
readr::write_rds(dff_1,"data-raw/dff_1.rds")
dff_2 <- purrr::map_df(list_of_files[1001:2000],ncdf_reader)
readr::write_rds(dff_2,"data-raw/dff_2.rds")
dff_3 <- purrr::map_df(list_of_files[2001:length(list_of_files)],ncdf_reader)
readr::write_rds(dff_3,"data-raw/dff_3.rds")

df1 <- readr::read_rds("data-raw/dff_1.rds")
df1 <- df1 |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df1)

df2 <- readr::read_rds("data-raw/dff_2.rds")
df2 <- df2 |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df2)

df3 <- readr::read_rds("data-raw/dff_3.rds")
df3 <- df3 |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df3)
dff <- rbind(df1,df2,df3)

dff <- dff |>
  dplyr::mutate(
    flag_br = def_pol(longitude, latitude, pol_br)
  )
dplyr::glimpse(dff)
```

O processamento acima poderá ser realizado com: **1. Abordagem com purrr::map_df + Divisão Automática**

```{r, eval = FALSE}
library(purrr)
library(dplyr)

# Divide os arquivos em lotes de tamanho fixo (ex.: 1000 por lote)
batch_size <- 1000
file_batches <- split(list_of_files, ceiling(seq_along(list_of_files) / batch_size))

# Processa todos os lotes de uma vez e salva com nomes dinâmicos
file_batches  |> 
  imap(~ {
    df <- map_df(.x, ncdf_reader) 
      filter(
        between(longitude, -80, -32),
        between(latitude, -34, 10)
      )
    saveRDS(df, paste0("data-raw/dff_", .y, ".rds"))  # .y = índice do lote
  })
```

**2. Abordagem com furrr (Paralelização)**

```{r, eval = FALSE}
library(furrr)  # Versão paralela do purrr
plan(multisession, workers = 4)  # Usa 4 núcleos da CPU

# Processamento paralelo
file_batches  |> 
  future_imap(~ {
    df <- future_map_df(.x, ncdf_reader) |>   # Leitura paralela
      filter(between(longitude, -80, -32), between(latitude, -34, 10))
    saveRDS(df, paste0("data-raw/dff_", .y, ".rds"))
  })
```

Preparando os polígonos geográficos das regiões brasileiras para análises espaciais, com um tratamento especial para ajustar os contornos.

```{r, eval = FALSE}
regiao <- geobr::read_region(showProgress = FALSE)
pol_norte <- regiao$geom |> purrr::pluck(1) |> as.matrix()
pol_nordeste <- regiao$geom |> purrr::pluck(2) |> as.matrix()
pol_sudeste <- regiao$geom |> purrr::pluck(3) |> as.matrix()
pol_sul <- regiao$geom |> purrr::pluck(4) |> as.matrix()
pol_centroeste<- regiao$geom |> purrr::pluck(5) |> as.matrix()

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Recriando o flag_nordeste
dff <- dff |>
  dplyr::mutate(
    flag_br = def_pol(longitude, latitude, pol_br),
    flag_nordeste = def_pol(longitude, latitude, pol_nordeste)
  )
```

```{r, eval = FALSE}
dff <- dff |>
  dplyr::filter(flag_br|flag_nordeste)
data_set <- readr::read_rds("data/nasa-xco2.rds") |>
  # tibble::tibble() |>
  dplyr::filter(
    xco2 > 0,
    xco2_quality_flag == 0) |>
    dplyr::mutate(
      path = stringr::str_remove(path, "data-raw/nc4/|\\.nc4"),
      date = lubridate::as_date(stringr::str_sub(path,12,17)),
      year = lubridate::year(date),
      month = lubridate::month(date),
      day = lubridate::day(date),
      .after = "time"
    )
dplyr::glimpse(data_set)

# # Criando a coluna para os estados.
# # carregando os poligonos e funções

#
# # Leitura do banco de dados
data_set <- readr::read_rds("data/nasa-xco2.rds")
#
# # Classificando pontos
state <- 0
x <- data_set |> dplyr::pull(longitude)
y <- data_set |> dplyr::pull(latitude)
for(i in 1:nrow(data_set)) state[i] <- get_geobr_state(x[i],y[i])
data_set <- data_set |> cbind(state)
# readr::write_rds(data_set,"data/nasa-xco2.rds")

dff <- readr::read_rds("../data/nasa-xco2.rds")
br |>
  ggplot2::ggplot() +
  ggplot2::geom_sf(fill="white", color="#FEBF57",
                   size=.15, show.legend = FALSE) +
  ggplot2::geom_point(data= dff |>
                        dplyr::sample_n(1000) |>
                        dplyr::filter(flag_br|flag_nordeste) ,
                      ggplot2::aes(x=longitude,y=latitude),
                      shape=3,
                      col="red",
                      alpha=0.2)
```

### Faxina e Processamento de Dados de CH~4~ Atmosférico (XCH4) Gosat

Buscamos todos arquivos NetCDF (.nc) no diretório e subdiretórios da pasta `data-raw` e retorna caminhos completos para processamento. Posteriormente a função de leitura, semelhante à anterior abre cada arquivo NetCD, extrai $7$ variáveis-chave para análise e fecha a conexão após leitura retornando um dataframe padronizado

Variáveis extraídas:

-   xch4: Concentração de metano (ppb)

-   xch4_quality_flag: Indicador de qualidade

-   xch4_incerteza: Margem de erro das medições

```{r, eval = FALSE}
## Lista com caminho e nomes dos arquivo a serem lidos - xco2
list_of_files <- list.files("data-raw/CH4_GOS_OCPR/",
                            pattern = ".nc",
                            full.names = TRUE,
                            all.files = TRUE,
                            recursive = TRUE)

## função para ler arquivos NCD4
ncdf_reader <- function(file_path){
  nc_file <- ncdf4::nc_open(file_path)
  df <- data.frame(
    "longitude"=ncdf4::ncvar_get(nc_file,varid="longitude"),
    "latitude"=ncdf4::ncvar_get(nc_file,varid="latitude"),
    "time"=ncdf4::ncvar_get(nc_file,varid="time"),
    "xch4"=ncdf4::ncvar_get(nc_file,varid="xch4"),
    "xch4_quality_flag"=ncdf4::ncvar_get(nc_file,varid="xch4_quality_flag"),
    "xch4_incerteza"=ncdf4::ncvar_get(nc_file,varid="xch4_uncertainty"),
    "path" = file_path
  )
  ncdf4::nc_close(nc_file)
  return(df)
}
```

**Processamento em Lote dos Dados**

Semelhante ao processamento anterior, seleciona apenas dados dentro da "caixa delimitadora" do Brasil e utiliza um filtro rápido antes da análise espacial precisa. Usa o polígono exato do Brasil (`{pol_br}`) para filtrar pontos costeiros/ilhas e retorma como resultado um dataframe unificado `(dff)` com apenas medições válidas. Cada lote preserva metadados originais (path do arquivo). A paralelização pode ser usada nesse momento.

```{r, eval = FALSE}
dff_1 <- purrr::map_df(list_of_files[1:1000],ncdf_reader)
readr::write_rds(dff_1,"data-raw/dff_1.rds")
dff_2 <- purrr::map_df(list_of_files[1001:2000],ncdf_reader)
readr::write_rds(dff_2,"data-raw/dff_2.rds")
dff_3 <- purrr::map_df(list_of_files[2001:3000],ncdf_reader)
readr::write_rds(dff_3,"data-raw/dff_3.rds")
dff_4 <- purrr::map_df(list_of_files[3001:length(list_of_files)],ncdf_reader)
readr::write_rds(dff_4,"data-raw/dff_4.rds")

df1 <- readr::read_rds("data-raw/dff_1.rds")
df1 <- df1 |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df1)

df2 <- readr::read_rds("data-raw/dff_2.rds") |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df2)

df3 <- readr::read_rds("data-raw/dff_3.rds") |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df3)

df4 <- readr::read_rds("data-raw/dff_4.rds") |>
  dplyr::filter(longitude >= -80 &
                  longitude <= -32,
                latitude >= -34 &
                  latitude <= 10)
dplyr::glimpse(df3)
dff <- rbind(df1,df2,df3,df4)

dff <- dff |>
  dplyr::mutate(
    flag_br = def_pol(longitude, latitude, pol_br)
  )
```

Extraímos os contornos das $5$ regiões do Brasil usando `{geobr}` e posteriormente ajustamos os polígonos para remover pontos a leste da longitude -34° (ilhas oceânicas) e corrigir distorções em áreas costeiras específicas. A saída é objeto `pol_br`, o polígono do Brasil ajustado e o `pol_nordeste`, o polígono do Nordeste ajustado.

```{r, eval = FALSE}
regiao <- geobr::read_region(showProgress = FALSE)
pol_norte <- regiao$geom |> purrr::pluck(1) |> as.matrix()
pol_nordeste <- regiao$geom |> purrr::pluck(2) |> as.matrix()
pol_sudeste <- regiao$geom |> purrr::pluck(3) |> as.matrix()
pol_sul <- regiao$geom |> purrr::pluck(4) |> as.matrix()
pol_centroeste<- regiao$geom |> purrr::pluck(5) |> as.matrix()

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Recriando o flag_nordeste
dff <- dff |>
  dplyr::mutate(
    flag_br = def_pol(longitude, latitude, pol_br),
    flag_nordeste = def_pol(longitude, latitude, pol_nordeste)
  )
```

Correções, - mantém apenas pontos dentro do Brasil (`flag_br`) ou Nordeste (`flag_nordeste`) removendo valores inválidos. Tratamento de Datas extrai informações temporais do caminho do arquivo:

```         
 - `date`: Data no formato `YYYY-MM-DD`  
 - `year`/`month`/`day`: Componentes separados  
```

Classificação por Estado pela função `get_geobr_state` para identificar o estado de cada coordenada resultado armazenado na coluna `state`.

-   Salva o dataset processado como `gosat-xch4.rds`\
-   Estrutura final inclui:
    -   Dados de concentração de CH4 validados\
    -   Informações geográficas (coordenadas + estado)\
    -   Metadados temporais completos

```{r, eval = FALSE}
data_set <- dff |>
  dplyr::filter(flag_br|flag_nordeste) |>
  dplyr::filter(
    xch4 > 0,
    xch4_quality_flag == 0) |>
    dplyr::mutate(
      path = stringr::str_remove(path, "data-raw/CH4_GOS_OCPR/20...|\\.nc"),
      date = lubridate::as_date(stringr::str_sub(path,27,34)),
      year = lubridate::year(date),
      month = lubridate::month(date),
      day = lubridate::day(date),
      .after = "time"
    )
dplyr::glimpse(data_set)

# Criando a coluna para os estados.
# carregando os poligonos e funções


# Classificando pontos
state <- 0
x <- data_set |> dplyr::pull(longitude)
y <- data_set |> dplyr::pull(latitude)
for(i in 1:nrow(data_set)) state[i] <- get_geobr_state(x[i],y[i])
data_set <- data_set |> cbind(state)
dplyr::glimpse(data_set)
# readr::write_rds(data_set,"data/gosat-xch4.rds")

dff <- readr::read_rds("../data/gosat-xch4.rds")
br |>
  ggplot2::ggplot() +
  ggplot2::geom_sf(fill="white", color="#FEBF57",
                   size=.15, show.legend = FALSE) +
  ggplot2::geom_point(data= dff |>
                        dplyr::sample_n(1000) |>
                        dplyr::filter(flag_br|flag_nordeste) ,
                      ggplot2::aes(x=longitude,y=latitude),
                      shape=3,
                      col="red",
                      alpha=0.2)
```

### Faxina e Processamento de Dados de SIF (*solar induced fluorescence*) OCO-2/NASA.

1.  **Conversão de Unidades e Datas**

    -   Converte XCO₂ para ppm (`xco2_moles_mole_1*1e06`)
    -   Extrai ano/mês/dia do timestamp

2.  **Cálculo de SIF**\
    Combina medidas de fluorescência em 757nm e 771nm numa métrica única

3.  **Filtro Geográfico**

    -   Prepara polígonos das regiões brasileiras\
    -   Remove pontos oceânicos/distantes\
    -   Cria flags para Brasil (`flag_br`) e Nordeste (`flag_nordeste`)

4.  **Visualização Rápida**

    -   Mapa com amostra de 1.000 pontos sobre o território brasileiro

5.  **Classificação por Estado**

    -   Atribui cada ponto a um estado usando coordenadas

```{r, eval=FALSE}
dff <- fco2r::oco2_br |>
  dplyr::mutate(
    xco2 = xco2_moles_mole_1*1e06,
    date = lubridate::ymd_hms(time_yyyymmddhhmmss),
    year = lubridate::year(date),
    month = lubridate::month(date),
    day = lubridate::day(date),
    sif = (
      fluorescence_radiance_757nm_idp_ph_sec_1_m_2_sr_1_um_1*2.6250912*10^(-19)  + 1.5*fluorescence_radiance_771nm_idp_ph_sec_1_m_2_sr_1_um_1* 2.57743*10^(-19))/2)

regiao <- geobr::read_region(showProgress = FALSE)
pol_norte <- regiao$geom |> purrr::pluck(1) |> as.matrix()
pol_nordeste <- regiao$geom |> purrr::pluck(2) |> as.matrix()
pol_sudeste <- regiao$geom |> purrr::pluck(3) |> as.matrix()
pol_sul <- regiao$geom |> purrr::pluck(4) |> as.matrix()
pol_centroeste<- regiao$geom |> purrr::pluck(5) |> as.matrix()

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Retirando alguns pontos
pol_br <- pol_br[pol_br[,1]<=-34,]
pol_br <- pol_br[!((pol_br[,1]>=-38.8 & pol_br[,1]<=-38.6) &
                     (pol_br[,2]>= -19 & pol_br[,2]<= -16)),]

pol_nordeste <- pol_nordeste[pol_nordeste[,1]<=-34,]
pol_nordeste <- pol_nordeste[!((pol_nordeste[,1]>=-38.7 & pol_nordeste[,1]<=-38.6) & pol_nordeste[,2]<= -15),]

# Recriando o flag_nordeste
dff <- dff |>
  dplyr::mutate(
    flag_br = def_pol(longitude, latitude, pol_br),
    flag_nordeste = def_pol(longitude, latitude, pol_nordeste)
  )

br |>
  ggplot2::ggplot() +
  ggplot2::geom_sf(fill="white", color="#FEBF57",
                   size=.15, show.legend = FALSE) +
  ggplot2::geom_point(data= dff |>
                        dplyr::sample_n(1000) |>
                        dplyr::filter(flag_br|flag_nordeste) ,
                      ggplot2::aes(x=longitude,y=latitude),
                      shape=3,
                      col="red",
                      alpha=0.2)

# Classificando pontos
data_set <- dff
state <- 0
x <- data_set |> dplyr::pull(longitude)
y <- data_set |> dplyr::pull(latitude)
for(i in 1:nrow(data_set)) state[i] <- get_geobr_state(x[i],y[i])
data_set <- data_set |> cbind(state)
dplyr::glimpse(data_set)
readr::write_rds(data_set,"../data/oco2-sif.rds")
```

## Faxina de Dados do APPEEARS (MODIS/Vegetação)

O **APPEEARS** (*Application for Extracting and Exploring Analysis Ready Samples*) é uma plataforma da **NASA** desenvolvida pela **Land Processes Distributed Active Archive Center (LP DAAC)** que facilita o acesso e o pré-processamento de dados de sensoriamento remoto, especialmente da coleção **MODIS** e outros produtos de observação da Terra.

**Objetivo**\
Este pipeline processa dados ecológicos do **MODIS** extraídos via **APPEEARS**, contendo métricas essenciais para monitoramento vegetativo:

**Variáveis Processadas**:\
- **ET** (Evapotranspiração) - **EVI** (Enhanced Vegetation Index)\
- **NDVI** (Normalized Difference Vegetation Index)\
- **FPAR** (Fraction of Absorbed Photosynthetically Active Radiation) - **LAI** (Leaf Area Index)

```{r, eval=FALSE}
# Carregar bibliotecas
library(tidyverse)
library(dplyr)

# Lista de arquivos
csv_files_list <- list.files("../data-raw/AppEEARS/", 
                       pattern = ".csv", 
                       full.names = TRUE)

# Empilhar arquivos 
appeears_stack <- purrr::map_dfr(csv_files_list, 
                           readr::read_csv)
```

Abaixo vamos selecionar camadas de interesses, renomear colunas, extraindo o ano e o mês das datas no formato YYYY-MM-DD, filtrar para nos anos que serão analisados e agrupar por lat e lon. Após o agrupamento é calculado a média das variáveis numéricas por mês e ano.

```{r, eval=FALSE}
appeears <- appeears_stack |> 
   dplyr::select(Latitude, Longitude, Date, MOD15A2H_061_Lai_500m, MOD15A2H_061_Fpar_500m, 
         MOD13Q1_061__250m_16_days_EVI, MOD13Q1_061__250m_16_days_NDVI, 
         MOD16A2_061_ET_500m) |>
  dplyr::rename(date = Date,
         lat = Latitude,
         lon = Longitude,
         lai = MOD15A2H_061_Lai_500m,
         fpar = MOD15A2H_061_Fpar_500m,
         evi = MOD13Q1_061__250m_16_days_EVI,
         ndvi = MOD13Q1_061__250m_16_days_NDVI,
         et = MOD16A2_061_ET_500m) |> 
  dplyr::mutate(year = lubridate::year(date),
                month = lubridate::month(date))  |> 
  dplyr::filter(
    year >= 2015 & year <= 2023
  ) |> 
  dplyr::group_by(lat, lon, year, month)  |> 
  dplyr::summarise(
    dplyr::across(c(fpar, lai, evi, ndvi, et), 
           ~ mean(., na.rm = TRUE),
           .names = "media_{.col}"),
    n_observacoes = dplyr::n(),
    .groups = 'drop'
  ) |> 
  dplyr::arrange(lat, lon, year, month)
# Valores NA na coluna ET (Dados de 2021 a 2024)

# Visualizar dados
dplyr::glimpse(appeears)
# View(appeears)
```

```{r, eval=FALSE}
# Verificacao de pontos Brasil Central

br_country <- geobr::read_country(showProgress = FALSE)

br_country |> 
  ggplot2::ggplot()+
  ggplot2::geom_sf()+
  ggplot2::geom_point(
    data=appeears |> 
      dplyr::filter(ano == 2018), 
      ggplot2::aes(lon,lat)
  )

appeears |> 
  dplyr::filter(ano == 2017) |> 
  ggplot2::ggplot(ggplot2::aes(lon,lat)) +
  ggplot2::geom_point()
# Gerar arquivo com os dados
# readr::write_rds(appeears, '../data/appeears-modis.rds')
```

### NASA-POWER

Criação de um grid de pontos sobre a região Centro-Oeste do Brasil. É utilizado pacotes geográficos como `{geobr}` e `{sf}`. Usamdo o pacote `{geobr}`, que fornece shapefiles (polígonos) oficiais do IBGE.

`st_union()` une os polígonos dos estados em um único polígono (uma geometria só). Isso evita sobreposição ou múltiplas fronteiras internas.

`st_make_grid()` cria uma grade (grid) de células retangulares ou quadradas. `cellsize = 0.5` define o tamanho das células (aqui: 0.5 graus de latitude/longitude). `what = "centers"` indica os centros dos quadrados, e não os quadrados em si.

`st_as_sf()` converte os pontos em um sf (formato espacial).

Esse tipo de grid é útil para:

-   Amostragem espacial;\
-   Modelagem de distribuição (e.g., interpolação);\
-   Análises ambientais ou climáticas em pontos regulares;\
-   Servir como base para extrair dados raster (NDVI, precipitação, etc.).

```{r, eval=FALSE}
# Lendo os estados brasileiros pelo pacote geobr
estados_br <- geobr::read_state(year = 2020, 
                                showProgress = FALSE)

# Filtrando estados de interesse (Centro-Oeste)
estados <- estados_br |> 
  dplyr::filter(name_region == 'Centro Oeste')

# Unindo polígono dos estados
pol_estados <- estados |> 
  sf::st_union()

# Criar grid de pontos
grid_pontos <- sf::st_make_grid(pol_estados, 
                            cellsize = 0.5, 
                            what = "centers") |> 
  sf::st_as_sf()
```

filtro espacial dos pontos do grid, mantendo apenas os que estão dentro da região Centro-Oeste, e depois extrai as coordenadas (latitude e longitude) para visualização com `ggplot2`.

`st_within(grid_pontos, pol_estados, sparse = FALSE)` cria uma matriz booleana dizendo quais pontos do grid estão dentro do polígono da região Centro-Oeste.

`sparse = FALSE`: retorna uma matriz lógica completa (TRUE/FALSE), em vez de uma lista esparsa.

`grid_pontos[...]`: usa essa matriz lógica para filtrar apenas os pontos `"TRUE"`, ou seja, os que estão dentro do polígono.

`st_coordinates()` extrai as coordenadas X (longitude) e Y (latitude) dos objetos sf com geometria do tipo ponto.

Pega a matriz coord e transforma em um data.frame com nomes amigáveis: lon (longitude), lat (latitude).

```{r, eval=FALSE}
# Filtrar pontos dentro do Centro-Oeste
pontos_filtro <- grid_pontos[sf::st_within(grid_pontos, 
                                       pol_estados, 
                                       sparse = FALSE),]

# Extrair coordenadas de cada ponto no grid
coord <- sf::st_coordinates(pontos_filtro)

# Gerando df com latitude e longitude
df_coords <- data.frame(lon = coord[,1], lat = coord[,2])

# Conferindo pontos
ggplot2::ggplot() +
    ggplot2::geom_sf(data = pol_estados, fill = "black", color = "black") +
    ggplot2::geom_point(data = df_coords, ggplot2::aes(x = lon, y = lat), color = "red", size = 1.5) + 
    ggplot2::theme_bw()
```

agora vamos baixar dados climáticos para cada ponto do grid

`try()` serve para tentar rodar um código que pode gerar erro, mas sem parar o script.

`power_data_download()` é uma função do pacote `{nasapower}`, que baixa dados meteorológicos da NASA POWER API.

`inherits(dw, "try-error")`: verifica se a tentativa anterior gerou erro.

O `! (negação)` faz com que, se não houver erro, o laço repeat seja interrompido com break.

Portanto para cada ponto `(i)` da grade de coordenadas:

```         
1-Tenta baixar os dados meteorológicos;

2-Se der erro, tenta de novo (infinitamente, até funcionar);

3-Se funcionar, sai do repeat e passa para o próximo ponto.
```

```{r, eval=FALSE}
# Baixar dados nasa power
for (i in 1:nrow(df_coords)) {
  repeat {
    dw <- try(
      nasapower::power_data_download(df_coords$lon[i], df_coords$lat[i],
                          startdate = '2015-01-01',
                          enddate = '2024-01-01')
    )
    if (!(inherits(dw, "try-error")))
      break
  }
}
```

Agora vamos ler todos os `.csv` gerados pelo loop anterior (um por ponto), juntar tudo num único data frame, limpar os nomes das colunas e salvar como `.rds`.

```{r, eval=FALSE}
# Criar banco de dados com os arquivos baixados
files_names <- list.files("../data-raw/nasa-power/", 
                          full.names = TRUE,
                          pattern = ".csv")

df_final <- purrr::map_dfr(files_names, read.csv)

# Salvar o banco de dados final
readr::write_rds(df_final |> 
                   janitor::clean_names(), 
                 '../data/nasa-power.rds')
```

## desmat_prodes

```{r, eval=FALSE}
library(terra)
library(raster)
library(dplyr)
library(ggplot2)
library(tidyr)
library(geobr)
library(sf)

# Carregar raster
r <- rast("../data-raw/desmat_prodes/prodes_br_2023.tif")   #colocar o caminho do arquivo raster
```

Criar subconjunto (subset) do raster para Brasil Central

```{r, eval=FALSE}
# Limites dos estados de interesse
estados <- read_state(year = 2020, 
                      code_state = "all", showProgress = FALSE)|> 
  filter(abbrev_state %in% c("MT", "MS", "GO"))

# Converter para o mesmo sistema de coordenadas do raster
estados <- st_transform(estados, crs(r))


# Criar o subset
r_brcentral <- crop(r, estados, mask=T)

plot(r_brcentral)

# Salvar
# writeRaster(subset, "data-raw/subsetbrcentral")
```


```{r, eval=FALSE}
legenda <- data.frame(
  valor = c(0, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 
            18, 19, 20,21, 22, 23,50, 51, 52, 53, 54, 55, 56, 57, 
            58, 59, 60, 61, 62, 63,91, 99, 100, 101),
  classe = c("d2000", "d2002", "d2004", "d2006", "d2007", "d2008", "d2009",
             "d2010", "d2011", "d2012", "d2013", "d2014", "d2015", "d2016",
             "d2017", "d2018", "d2019", "d2020", "d2021", "d2022", "d2023",
             "r2010", "r2011", "r2012", "r2013", "r2014", "r2015", "r2016",
             "r2017", "r2018", "r2019", "r2020", "r2021", "r2022", "r2023",
             "H", "C", "V", "N")
)

## h = hidrologia; c = nuvem; v = vegetação nativa e n = não vegetada

r_central_Class <- r_brcentral

levels(r_central_Class) <- legenda

r_central_Class |> is.factor()

```

```{r, eval=FALSE}
plot(r_central_Class)
```


```{r, eval=FALSE}
df <- r_central_Class |> 
  freq() |> 
  as_tibble() |> 
  mutate(
    area_Mha = ((count*30^2)*1e-4)/1e6,
    ano = stringr::str_sub(value,start=2,end=5),
    classe = stringr::str_sub(value,start=1,end=1),
    classe= case_when(
      classe == "d"~"Desmatamento",
      classe == "r"~"Reflorestamento",
      classe=="H"~"Hidrografia",
      classe=="C"~"Nuvem",
      classe=="V"~"Vegetação Nativa",
      classe=="N"~"Não Vegetada",
      .default = "Não Observada"
    )
  )

```


```{r, eval=FALSE}
df |> 
  filter(classe=="Desmatamento") |> 
  ggplot(aes(x=as.numeric(ano),y=area_Mha))+
  geom_col()+
  labs(y="Area Desmatada (M ha)",
       x='')


df |> 
  filter(classe=="Desmatamento") |> 
  mutate(
    area_acumulada = cumsum(area_Mha)
  ) |> 
  ggplot(aes(x=as.numeric(ano),y=area_acumulada))+
  geom_col()+
  labs(y="Area Desmatada Acumulada (M ha)",
       x="")

write_rds(df, "../data/prodes_brc.rds")
```

```{r, eval=FALSE}
categorias <- levels(r_central_Class)[[1]]$classe
lvls <- levels(r_central_Class)[[1]]

dcat <- grep("^d",categorias,value=T)
dcod <- lvls[lvls$classe %in% dcat, "classe"]

desma_raster <- ifel(r_central_Class %in% dcod, r_central_Class,NA)
```


```{r, eval=FALSE}
plot(desma_raster |> sum())
```
